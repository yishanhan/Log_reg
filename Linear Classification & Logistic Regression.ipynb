{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification & Logistic Regression\n",
    "\n",
    "Let's briefly review some topics we learned.\n",
    "\n",
    "#### Linear Regression Models\n",
    "\n",
    "We wanted to find $\\theta$ that minimizes our loss function, $J(\\theta) = \\sum^{n}_{i=1} (h(\\theta) - y)^2$, where \n",
    "$h(\\theta) = x^T \\theta$, and $x$ is one of $n$ training samples with dimension $d+1$ after adding a bias term, and $\\theta$ is our weights also with dimension $d+1$. To do this, we performed gradient descent, a process that finds the weights that reduces our loss.\n",
    "\n",
    "<img src=\"http://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png\">\n",
    "\n",
    "This involved computing the derivative of the loss with respect to $\\theta$, $\\frac{dJ}{d\\theta}$, updating $\\theta = \\theta - \\alpha * \\frac{dJ}{d\\theta}$, and performing this for some number of iterations.\n",
    "\n",
    "There were 3 types of gradient descent algorithms we looked at. <b>Batch gradient descent</b> involves updating $\\theta$ after looking at all $n$ points for each iteration. Looks through all the X[i] before updating the thetas to the list and then goes to the next iteration. Formally, each iteration is written as $$\\theta = \\theta - \\frac{\\alpha}{n} * \\sum_{i=1}^{n} (h(x^{(i)}) - y)x^{(i)}$$, where $\\alpha$ is the learning rate. \n",
    "\n",
    "<b>Stochastic gradient descent</b> involves updating $\\theta$ on each sample, where each iteration is formulated as this: $$\\theta = \\theta - \\alpha * (h(x^{(i)}) - y)x^{(i)}$$ for $i=1...n$. \n",
    "\n",
    "We then discovered that each update in batch gradient descent takes a while and stochastic gradient descent updates can be volatile, so we combined batch and stochastic gradient descent to make <b>mini-batch gradient descent</b>, which is to look at a batch size samples before updating $\\theta$. Each iteration is then written like this $$\\theta = \\theta - \\alpha * \\sum_{i=j*bs}^{min((j + 1) * bs, n)} (h(x^{(i)}) - y)x^{(i)}$$ for $j = 1...(int(\\frac{n}{bs}) - 1),int(\\frac{n}{bs})$, where $bs$ is the batch size. (Remember, $int(3.8)=3$.) For example, if $bs=10$ and $n=55$, then the equation looks like this $$\\theta = \\theta - \\alpha * \\sum_{i=j*10}^{min((j + 1) * 10, 55)} (h(x^{(i)}) - y)x^{(i)}$$ for $j = 1,2,3,4,5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "<img src=\"http://pingax.com/wp-content/uploads/2014/05/underfitting-overfitting.png\">\n",
    "\n",
    "Suppose we can use any degree polynomial as our model to best fit to the data. Then, there may be a high-degree polynomial that can perfectly fit to our data such that $J(\\theta)=0$ or come very very close such that $J(\\theta) < \\epsilon$, where $\\epsilon$ is some small number. But, when we get a new sample that we want to predict, our prediction may be inaccurate because our trained model has <b>overfitted</b>. We say that our model fails to <b>generalize</b> to new data. The opposite is <b>underfitting</b>, which happens when our model is not complex enough to represent the complexities of the data.\n",
    "\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\">\n",
    "\n",
    "Let me briefly explain the <b>bias vs variance tradeoff</b>. From wikipedia: \n",
    "\n",
    "* The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "* The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.\n",
    "\n",
    "From Udacity: \n",
    "* High bias - one that practically ignores the data and has no capacity to learn anything from the data\n",
    "* High Variance - extremely perceptive to data and can only replicate things that they have seen before. Is not able to learn from things they have not seen before \n",
    "\n",
    "Here's an intuitive explanation from <a href=\"https://www.quora.com/What-is-an-intuitive-explanation-for-bias-variance-tradeoff\">Quora</a> (the first response is also good):\n",
    "* Very simple models don't change much from dataset to dataset (low variance), but they also can't get terribly close to the truth (high bias).  Very complicated models can easily be right on average (low bias), but they tend to change much more with small changes in the input (high variance).  Somewhere in the middle there's a sweet spot, with a relatively accurate model that's not too sensitive to the data.\n",
    "\n",
    "How do we know if our model is underfitting or overfitting? \n",
    "\n",
    "If Underfit(Bias):\n",
    "* Jtrain(theta) will be high \n",
    "* Jcrossvalidation(theta) approx equal to Jtrain(theta)\n",
    "\n",
    "If overfit(variance):\n",
    "* Jtrain(theta) will be low\n",
    "* Jcrossvalidation(theta) >> Jtrain(theta) \n",
    "\n",
    "\n",
    "To reach the optimum balanace between bias vs variance, and, similarly, overfitting vs underfitting, we use regularization. Regularization is a way for us to punish more complex models.\n",
    "\n",
    "There are 3 common forms of regularization, L0, L1, and L2, with L2 being the most common and practical. We'll only explain L2.\n",
    "\n",
    "L2 regularization involves adding $\\lambda||\\theta||^2$ to the cost function. Recall that $||\\theta||^2 = \\theta_0^2 + ... + \\theta_{d+1}^2$. If one term of $\\theta$ were large, then our cost would be greatly increased. So, L2 regularization has the effect of forcing the $\\theta$'s to be small. How small depends on the regularization coefficient and hyperparameter $\\lambda$. We find the best $\\lambda$ by testing a variety $\\lambda$s and seeing which one leads to the optimal training/test error.\n",
    "\n",
    "Everything else remains the same. During gradient descent, we take the derivative of $J(\\theta) = \\sum^{n}_{i=1} (h(\\theta) - y)^2 + \\lambda||\\theta||^2$, which is, for batch gradient descent, $$\\theta = \\theta - \\frac{\\alpha}{n} * \\sum_{i=1}^{n} (h(x^{(i)}) - y)x^{(i)} + 2\\lambda\\theta$$. Since $\\lambda$ is a constant, we can merge the 2 into \\lambda to make $$\\theta = \\theta - \\frac{\\alpha}{n} * \\sum_{i=1}^{n} (h(x^{(i)}) - y)x^{(i)} + \\lambda\\theta$$. This is similar for minibatch and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Classification\n",
    "\n",
    "<b>Classification</b> is the problem of predicting what our target sample is when our targets are categories, like dead vs. alive, boy vs. girl, or red, blue, or orange. We can quickly adapt our linear regression model to linear classification.\n",
    "\n",
    "Suppose, we have two classes, 1 and -1. Let's redefine the problem in terms of predicting a sample $x$, using the same $h$ we had before. If  $h(x) >= 0$, then we say that $x$ belongs to class 1, and if $h(x) < 0$, then we say that $x$ belongs to -1. Our loss function remains the same: $J(\\theta) = (y - h(x))^2$.\n",
    "\n",
    "If you think about it, we can rewrite our loss function in terms of $z=yh(x)$ and $J(\\theta) = (1 - z)^2$. If the actual class $y=1$ and $h(x) > 0$, then $z > 0$, which means our prediction is correct. If the actual class $y=-1$ and the prediction $h(x) < 0$, then $z > 0$, which means the prediction is still correct. On the flip side, if the actual class is $y=-1$ and $h(x) > 0$, then $z < 0$, which means our prediction is incorrect. At the end of the day, it's an just another formulation of the loss function.\n",
    "\n",
    "The new derivative is just $\\frac{dJ}{d\\theta} = 2(1 - yh(x))x = 2(1-z)x$ for a single training sample. Because we have a learning rate constant $\\alpha$, we can just exclude the 2 when we're using the derivative in gradient descent.\n",
    "\n",
    "And, everything else about gradient descent remains the same. Now, we have just covered linear classification. Now, let's start coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic\n",
    "\n",
    "We'll tackle the problem of predicting the survival of passengers on the Titanic.\n",
    "\n",
    "The features we'll use are:\n",
    "* Age\n",
    "* fare\n",
    "\n",
    "These features are continuous, compared to others like passenger class, gender, number of siblings on board, etc.\n",
    "\n",
    "pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125900  73800 152100 500001 316700]\n",
      "(19440, 8)\n",
      "(19440,)\n"
     ]
    }
   ],
   "source": [
    "HOUSING_DATA_FILE = \"housing_data.mat\"\n",
    "data = loadmat(HOUSING_DATA_FILE)\n",
    "X_train, X_val = data[\"Xtrain\"], data[\"Xvalidate\"]\n",
    "y_train, y_val = data[\"Ytrain\"], data[\"Yvalidate\"]\n",
    "y_train = y_train.reshape((y_train.shape[0]))\n",
    "y_val = y_val.reshape((y_val.shape[0]))\n",
    "print y_val[:5]\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def h(x, theta):\n",
    "    return np.dot(x, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    return np.sum((h(X, theta) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_cost(X_train, y_train, np.ones(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, theta, alpha=0.1, num_iters=100):\n",
    "    costs, thetas = [], []\n",
    "    for k in xrange(num_iters):\n",
    "        for i in xrange(X.shape[0]):\n",
    "            theta -= alpha * (y[i] - h(X[i], theta)) * X[i]\n",
    "            thetas.append(theta)\n",
    "            cost = compute_cost(X, y, theta)\n",
    "            costs.append(cost)\n",
    "            if i % 10 == 0:\n",
    "                print \"Iteration {0} [{1}/{2}]: Cost={3} Theta={4}\".format(k, i, X.shape[0], cost, theta)\n",
    "    return theta, costs, thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta = np.random.rand(8)\n",
    "print theta\n",
    "stochastic_gradient_descent(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(X_train[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
