{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifying Digits\n",
    "\n",
    "In this notebook, we'll be tackling the problem of classifying digits from images from the age-old MNIST dataset. We'll learn about how take raw data and run it through a pre-built classifier. In the process, we'll talk about cross validation and the sklearn library for python.\n",
    "\n",
    "<img src=\"http://simonwinder.com/wp-content/uploads/2015/07/mnistExamples.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Add all your imports here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "from sklearn import svm\n",
    "\n",
    "# Your data file\n",
    "TRAIN = \"train.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided in a .mat file, which is typically used by MATLAB. However, a python library called scipy can help you load the file. Google for which method to call, add the required import, then call the function below.\n",
    "\n",
    "You'll need to run `pip install scipy` to use scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def get_data(mat_file):\n",
    "    data = loadmat(mat_file)\n",
    "    train_images = data[\"train_images\"]\n",
    "    train_labels = data[\"train_labels\"].reshape((data[\"train_labels\"].shape[0],))\n",
    "    return train_images, train_labels\n",
    "\n",
    "train_images, train_labels = get_data(TRAIN)\n",
    "# this should be True, each image is 28x28 and there are 60000 train images\n",
    "print train_images.shape == (28, 28, 60000)\n",
    "# should be a single vector of size 60000, each item is a number 0-9\n",
    "print train_labels.shape == (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of our data being 28x28x60000, we would rather have our data be nxd, or 60000x784. Use `np.reshape` to convert our data to 784x60000, then use `np.swapaxes` to swap the axes around to make it 60000x784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def reshape_images(images):\n",
    "    \"\"\"\n",
    "        Images is 28 x 28 x 60000 array\n",
    "    \"\"\"\n",
    "    # 784 x 60000\n",
    "    pixels_features_to_samples = np.reshape(images, (784, images.shape[2]))\n",
    "    # 60000 x 784\n",
    "    samples_to_features = np.swapaxes(pixels_features_to_samples, 0, 1)\n",
    "    return samples_to_features\n",
    "\n",
    "train_images = reshape_images(train_images)\n",
    "print train_images.shape == (60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, I've included the following function so that you can visualize an image from the data. Often, it helps to know what your data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115d00590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is:  3\n"
     ]
    }
   ],
   "source": [
    "def show_image(train_image_sample):\n",
    "    rs = np.reshape(train_image_sample, (28, 28))\n",
    "    plt.imshow(rs, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "show_image(train_images[23523])\n",
    "print \"Image is: \", train_labels[23523]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now for the actual machine learning part. We're going to use a LinearSVC to classify our digits. In the past, we've only worked with only 2 classes in 2 dimensions, trying to find the line that best separates them. In the multi-dimensional case, LinearSVC will try to find the best line that separates 0 from everything else, then 1 from everything else, and so on. Thus, we'll have 10 different decision boundaries, but all of that happens under the hood so we don't really need to look into details for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to randomly shuffle the data. If you haven't realized, the first several thousand are 0's, the second are 1's, and so on. If we don't shuffle the data, we won't learn properly. So, to shuffle the data, instead of using `np.random.shuffle` on the data, we want to get shuffled indicies. So, if we only have 5 samples of data, we want to get shuffled indices such as [3,2,5,0,1] so that we can do `train_data[3,2,5,0,1]` and `train_labels[3,2,5,0,1]`. Look at the `random.sample` method. It takes in a list, which you can pass in a list or `range` of numbers between [0, 60000) and request 60000 numbers without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_indices(lst):\n",
    "    return random.sample(lst, len(lst))\n",
    "\n",
    "train_random_indices = random_indices(range(60000))\n",
    "X, y = train_images[train_random_indices], train_labels[train_random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to separate our data and labels into two groups, training and validation. We fit our data to the training data and check its performance on the validation set. If training accuracy is higher than our validation accuracy, it might mean that we have overfitted to the data. Generally, we want them to be similar. Implement the following function. Split is a number 0 < split < 1. If split is .75, then the first 3/4 of train_images is X_train and the last 1/4 is X_val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def get_train_val_split(X, y, split):\n",
    "    num_train = int(X.shape[0] * split)\n",
    "    return X[:num_train], y[:num_train], X[num_train:], y[num_train:]\n",
    "\n",
    "X_train, y_train, X_val, y_val = get_train_val_split(X, y, 0.05)\n",
    "print X_train.shape == (45000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our data on the training set. Use `svm.LinearSVC` with parameter C=lambda defined in the method parameter. Use the `clf.fit` method to fit the data to the X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, C=1):\n",
    "    lin_clf = svm.LinearSVC(C=C)\n",
    "    lin_clf.fit(X_train, y_train)\n",
    "    return lin_clf\n",
    "\n",
    "lin_clf = train(X_train, y_train, C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the accuracy of the model. Use `clf.predict(X_val)` to get a `y_pred` and compare it with `y_val`. Return the number of predictions correct over the total number of items in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75228070175438599"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(clf, X_val, y_val):\n",
    "    y_pred = clf.predict(X_val)\n",
    "    return sum(y_val == y_pred) * 1. / X_val.shape[0]\n",
    "    \n",
    "validate(lin_clf, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do cross validation. Cross validation is the process of splitting up the training data into $k$ groups, training on $k - 1$ groups and validating on the remaining group. We do this $k$ times, with a different group used as the validation set at each time. Then, we take the $k$ individual accuracies and average them to get a cross validation score.\n",
    "\n",
    "Now why do we use cross validation? A simple answer would be that we would like to see how our model generalizes to an independent dataset. Obviously, we would like to see if we're overfitting. In addition, we can use cross validation to help us choose our hyperparameters. A <b>hyperparameter</b> is like parameters that we can manually change. For example, the model we use, the constant lambda (or C in this case), for polynomial models, the max degree of the polynomial, etc. There are many hyperparameters and we can run cross validation on each set of hyperparameters and compare them to get the best hyperparameters.\n",
    "\n",
    "So, how do we choose $k$? There isn't a really good answer and if you search it up, it's usually just what people feel like using. In general, if you have very little data, maybe you would consider leave one out cross validation, where if you have $n$ samples then your $k=n$, so you have n groups, or each group has 1 sample. Otherwise, it's safe to use $k=10$.\n",
    "\n",
    "Let's get our data again and randomize it. Then implement cross validation. You should be able to use the `train` and `validate` methods you implemented before. We'll only use 5000 samples because training takes a long time. The hard part is to split the 5000 samples and labels into 2 groups, 1 composed of 4500 items and the other with 500 items. \n",
    "\n",
    "Hint: Use a range(X.shape[0]) to get a list of numbers between [0, X.shape[0]) to use as indicies, then split the list into 2 groups, training and validation, for each iteration in k iterations. Then do X_train = X[train_indices], and X_val = X[val_indices], and do the same for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_random_indices = random_indices(range(60000))\n",
    "X, y = train_images[train_random_indices], train_labels[train_random_indices]\n",
    "# Take the first 5000 randomized samples\n",
    "X, y = X[:5000], y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for iteration 0 is 0.752\n",
      "Accuracy for iteration 1 is 0.774\n",
      "Accuracy for iteration 2 is 0.758\n",
      "Accuracy for iteration 3 is 0.804\n",
      "Accuracy for iteration 4 is 0.784\n",
      "Accuracy for iteration 5 is 0.794\n",
      "Accuracy for iteration 6 is 0.794\n",
      "Accuracy for iteration 7 is 0.776\n",
      "Accuracy for iteration 8 is 0.754\n",
      "Accuracy for iteration 9 is 0.732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7722"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_validation(X, y, k=10):\n",
    "    accuracies = []\n",
    "    split_size = X.shape[0] / k\n",
    "    indices = range(X.shape[0])\n",
    "    for i in range(k):\n",
    "        train_indices = indices[i * split_size: (i + 1) * split_size]\n",
    "        val_indices = indices[:i * split_size] + indices[(i + 1) * split_size:]\n",
    "        X_val, y_val = X[train_indices], y[train_indices]\n",
    "        X_train, y_train = X[val_indices], y[val_indices]\n",
    "        clf = train(X_train, y_train)\n",
    "        accuracy = validate(clf, X_val, y_val)\n",
    "        accuracies.append(accuracy)\n",
    "        print \"Accuracy for iteration {0} is {1}\".format(i, accuracy)\n",
    "    return sum(accuracies) * 1. / len(accuracies)\n",
    "\n",
    "cross_validation(X, y, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reimplement `train` so we can pass in any model. Also, instead of using svm.LinearSVC, we will now use svm.SVC and define our kernel and other hyperparameters in a dictionary. This way, we can just through a list of hyperparameter dictionary and run cross validation. You can look up what `**` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "def train(clf, X_train, y_train, C=1):\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "clf = svm.LinearSVC(C=1)\n",
    "print clf\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "print clf\n",
    "hyperparams = {'kernel': 'linear', 'C': 1}\n",
    "clf = svm.SVC(**hyperparams)\n",
    "print clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify cross validation to use the new `train` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(clf, X, y, k=10):\n",
    "    accuracies = []\n",
    "    split_size = X.shape[0] / k\n",
    "    indices = range(X.shape[0])\n",
    "    for i in range(k):\n",
    "        train_indices = indices[i * split_size: (i + 1) * split_size]\n",
    "        val_indices = indices[:i * split_size] + indices[(i + 1) * split_size:]\n",
    "        X_val, y_val = X[train_indices], y[train_indices]\n",
    "        X_train, y_train = X[val_indices], y[val_indices]\n",
    "        clf = train(clf, X_train, y_train)\n",
    "        accuracy = validate(clf, X_val, y_val)\n",
    "        accuracies.append(accuracy)\n",
    "        print \"Accuracy for iteration {0} is {1}\".format(i, accuracy)\n",
    "    return sum(accuracies) * 1. / len(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test several sets hyperparameters to see which one's best. Do add more hyperparameters to the list. This is for you to explore which hyperparameters is best. I don't know which is, so you tell me.\n",
    "\n",
    "Refer to <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">SVC documentation</a> to learn which hyperparameters are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for iteration 0 is 0.87\n",
      "Accuracy for iteration 1 is 0.884\n",
      "Accuracy for iteration 2 is 0.868\n",
      "Accuracy for iteration 3 is 0.888\n",
      "Accuracy for iteration 4 is 0.886\n",
      "Accuracy for iteration 5 is 0.874\n",
      "Accuracy for iteration 6 is 0.892\n",
      "Accuracy for iteration 7 is 0.858\n",
      "Accuracy for iteration 8 is 0.866\n",
      "Accuracy for iteration 9 is 0.864\n",
      "{'kernel': 'linear', 'C': 1}\n",
      "Cross validation accuracy is 0.875\n",
      "Accuracy for iteration 0 is 0.94\n",
      "Accuracy for iteration 1 is 0.936\n",
      "Accuracy for iteration 2 is 0.922\n",
      "Accuracy for iteration 3 is 0.94\n",
      "Accuracy for iteration 4 is 0.94\n",
      "Accuracy for iteration 5 is 0.94\n",
      "Accuracy for iteration 6 is 0.928\n",
      "Accuracy for iteration 7 is 0.926\n",
      "Accuracy for iteration 8 is 0.94\n",
      "Accuracy for iteration 9 is 0.928\n",
      "{'kernel': 'poly', 'C': 1, 'degree': 2}\n",
      "Cross validation accuracy is 0.934\n"
     ]
    }
   ],
   "source": [
    "h = [{'kernel': 'linear', 'C': 1}, {'kernel': 'poly', 'degree': 2, 'C': 1}]\n",
    "for hyperparams in h:\n",
    "    clf = svm.SVC(**hyperparams)\n",
    "    acc = cross_validation(clf, X, y, k=10)\n",
    "    print hyperparams\n",
    "    print \"Cross validation accuracy is {0}\".format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
